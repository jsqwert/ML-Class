{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ibf\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.017 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "jieba.add_word('郑秋冬')\n",
    "jieba.add_word('离境天')\n",
    "jieba.suggest_freq('离境天', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#第一个文档分词#\n",
    "with open('./datas/nlp_1.txt',encoding='utf-8') as f:\n",
    "    document = f.read()\n",
    "    document_cut = jieba.cut(document)\n",
    "    #print  ' '.join(jieba_cut)\n",
    "    result = ' '.join(document_cut)\n",
    "    with open('./datas/nlp_1_cut.txt', 'w',encoding='utf-8') as f2:\n",
    "        f2.write(result)\n",
    "#第二个文档分词#\n",
    "with open('./datas/nlp_2.txt',encoding='utf-8') as f:\n",
    "    document = f.read()\n",
    "    document_cut = jieba.cut(document)\n",
    "    #print  ' '.join(jieba_cut)\n",
    "    result = ' '.join(document_cut)\n",
    "    with open('./datas/nlp_2_cut.txt', 'w',encoding='utf-8') as f2:\n",
    "        f2.write(result)\n",
    "#第三个文档分词#\n",
    "with open('./datas/nlp_3.txt',encoding='utf-8') as f:\n",
    "    document = f.read()\n",
    "    jieba.load_userdict(\"./datas/01mydict.txt\")\n",
    "    document_cut = jieba.cut(document)\n",
    "    #print  ' '.join(jieba_cut)\n",
    "    result = ' '.join(document_cut)\n",
    "    with open('./datas/nlp_3_cut.txt', 'w',encoding='utf-8') as f2:\n",
    "        f2.write(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿ 据 典籍 记载 ， 巫族 隐居 世外 密林 离镜天 ， 自古 不 入 尘世 ， 巫族 之 人 凝聚 万物之灵 ， 匡扶 皇室 明君 ， 以 守护 四方 百姓 为 己任 ， 今日 便是 离镜天 执事 巫女 考核 的 日子 ， 只有 成为 执事 巫女 才能 走出 离镜 天去 守护 皇家 和 这 天下 。 \n",
      " 　 　 清净 的 山崖 间 ， 巫族 长老 桃 殀 着 一 袭 白衣 ， 手执 棋子 与 师兄 昔邪 博弈 ， 举手投足 间 银蝶 飞舞 ， 淡眉若 秋水 ， 玉肌 伴 轻风 ， 尽显 高贵 清丽 ， 她 知 师兄 担心 自己 的 徒儿 凤卿 尘 ， 却 只是 静观其变 。 \n",
      " 　 　 离镜 天内 ， 风景如画 ， 数名 紫衣 女子 手攀 绿藤 ， 竞相奔走 ， 使出 浑身解数 应对 这 危机重重 的 考核 。 然 陆续 有人 落入 陷阱 ， 电光石火 间 只见 一 紫衣 女子 点尘 不 惊 ， 踏风 而 来 ， 于 最后 一刻 成功 通关 。 女子 秀发 如瀑 ， 肤若 凝脂 ， 修眉入 鬓 ， 樱唇 嫣然 ， 这 便是 凤卿 尘 。 几经 波折 ， 卿 尘 和 另外 两名 女子 来到 了 最后 一关 ， 巫族 以 通晓 天地 而立 ， 前 两名 在 这 灼灼 桃花 间 找出 水源 者 便 为 通关 。 \n",
      " 　 　 十里 沙场 ， 三军 肃穆 ， 皇帝 立于 高处 静看 皇子 们 英勇 夺弓 ， 争夺 中一玄甲 黑衣 的 男子 脱颖而出 拔得 头筹 ， 这 男子 剑眉 星目 ， 英气逼人 ， 正是 率领 玄甲军 屡立 战功 的 四 皇子 元凌 。 皇上 对 其 赞赏 有加 ， 要 其 与 七 皇子 元湛 等 共入 军阵 夺金 符 。 \n",
      " 　 　 军阵 中 硝烟弥漫 ， 危机四伏 ， 元凌携 十一 皇子 元澈 奋勇 击敌 取得 锦囊 ， 却 发现 里面 不是 金符 ， 正是 失窃 已久 的 九城 兵符 。 正在 这时 九 皇子 元溟 赶到 ， 称四 皇子 元凌 偷盗 兵符 人赃俱获 ， 奉 皇上 秘旨 擒拿 回宫 。 这 摆明 了 是 栽赃 陷害 ， 元凌 却 了然 这是 父皇设 下 的 局 ， 所谓 君要 臣 死 ， 臣 不得不 死 ， 但元澈 却 还 在 义正 严辞 与 元 溟 争辩 ， 势必 要 站 在 四哥 一边 。 \n",
      " 　 　 元凌 被 逼 至 悬崖 走投无路 ， 其实 功高震主 ， 容不下 元凌 的 又 何止 是 皇上 ， 但 他 实在 无法 想象 ， 如此 构陷 手足 兄弟 之人 怎能 治理 这大魏 天下 。 元凌 磊落 ， 无畏 无惧 ， 眸光 沉沉 道 ， “ 归离 剑 下 从 无降 将 ， 生死 自当 由 我 自己 决定 ， 断 不会 受 任何人 摆布 。 ” 转身 便 决然 跳 下 悬崖 ， 与此同时 一支 利箭 破风 而 来 射入 元凌 胸口 ， 在 元澈 的 呼喊声 中 ， 元凌 无力 地 坠入 了 崖 底 。 \n",
      " 　 　 此时 凤卿 尘 正在 崖底 寻找 水源 ， 她 一 转身 ， 看见 元凌正 急速 坠落 ， 便 伸手 施法 相救 ， 走近 一看 ， 那 男子 身形 挺拔 ， 只是 薄唇 紧 抿 ， 显然 伤得 不轻 。 卿尘 警告 男子 离镜天 禁地 外人 不得 入内 ， 劝 他 速速 离开 ， 元凌 闻言 抓住 卿尘 ， 知 她 是 巫族 中人 ， 只是 话 没 说完 便 晕 了 过去 。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n"
     ]
    }
   ],
   "source": [
    "with open('./datas/nlp_1_cut.txt',encoding='utf-8') as f3:\n",
    "    cut1 = f3.read()\n",
    "# print(cut1)\n",
    "with open('./datas/nlp_2_cut.txt',encoding='utf-8') as f4:\n",
    "    cut2 = f4.read()\n",
    "# print(cut2)\n",
    "with open('./datas/nlp_3_cut.txt',encoding='utf-8') as f5:\n",
    "    cut3 = f5.read()\n",
    "\n",
    "print(cut3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#从文件导入停用词表\n",
    "stpwrdpath = \"./datas/stop_words.txt\"\n",
    "stpwrd_dic = open(stpwrdpath, 'rb')\n",
    "stpwrd_content = stpwrd_dic.read()\n",
    "#将停用词表转换为list  \n",
    "stpwrdlst = stpwrd_content.splitlines()\n",
    "stpwrd_dic.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 0 0]\n",
      " [1 1 2 ..., 3 1 1]\n",
      " [0 0 0 ..., 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "corpus = [cut1,cut2,cut3]\n",
    "CV = CountVectorizer(stop_words=stpwrdlst)\n",
    "CTF = CV.fit_transform(corpus)\n",
    "print(CTF.toarray())\n",
    "words=CV.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_topics=2,learning_method='batch',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "lda_result = lda.fit_transform(CTF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2.57302175e-03   9.97426978e-01]\n",
      " [  9.99099666e-01   9.00334145e-04]\n",
      " [  1.73681106e-03   9.98263189e-01]]\n",
      "[[ 1.49996064  1.49996064  2.49995958 ...,  3.49995936  1.49996064\n",
      "   1.49996064]\n",
      " [ 0.50003936  0.50003936  0.50004042 ...,  0.50004064  0.50003936\n",
      "   0.50003936]]\n"
     ]
    }
   ],
   "source": [
    "print(lda_result)\n",
    "print(lda.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
